{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load latest execution full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_654118/1990568059.py:1: DtypeWarning: Columns (58,76,78,100,153) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  full = pd.read_csv(\"Full/full_3.csv\", index_col=0)\n"
     ]
    }
   ],
   "source": [
    "full = pd.read_csv(\"Full/full_3.csv\", index_col=0)\n",
    "full = full.drop(['dram_read_bytes (dev_approximate_gravity)'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load selected Profiling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['gld_transactions (dev_approximate_gravity)',\n",
    " 'thread_inst_executed (dev_approximate_gravity)',\n",
    " 'inst_compute_ld_st (correct_particles)',\n",
    " 'active_warps_pm (correct_particles)',\n",
    " 'global_store_requests (correct_particles)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 1: intermediate model (Dtree->Dtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Split data for intermediate model training\n",
    "X = full[input_features]\n",
    "y_high_variance = full[selected_features]\n",
    "\n",
    "# Train an intermediate model for each high-variance feature\n",
    "intermediate_model = DecisionTreeRegressor()  # Using Linear Regression as an example\n",
    "intermediate_model.fit(X, y_high_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate high-variance feature predictions from the intermediate model\n",
    "predicted_high_variance = intermediate_model.predict(X)\n",
    "predicted_high_variance_df = pd.DataFrame(predicted_high_variance, columns=selected_features)\n",
    "\n",
    "# Combine core features with the predicted high-variance features for main model training\n",
    "X_full = pd.concat([X, predicted_high_variance_df], axis=1)\n",
    "y_exec_time = full['exec_time_avg']\n",
    "\n",
    "# Split data for main model training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_exec_time, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the main model with both core and high-variance features\n",
    "main_model = DecisionTreeRegressor(random_state=42)\n",
    "main_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict high-variance features from the core input features in the test set\n",
    "predicted_high_variance_test = intermediate_model.predict(X_test[input_features])\n",
    "predicted_high_variance_test_df = pd.DataFrame(predicted_high_variance_test, columns=selected_features)\n",
    "\n",
    "# Combine core inputs and predicted high-variance features for the final prediction\n",
    "X_test_full = pd.concat([X_test[input_features].reset_index(drop=True), predicted_high_variance_test_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Make final predictions\n",
    "y_pred = main_model.predict(X_test_full)\n",
    "\n",
    "# Display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'N': X_test['N'],\n",
    "    'theta': X_test['theta'],\n",
    "    'dt': X_test['dt'],\n",
    "    'Multiprocessors (SMs)': X_test['Multiprocessors (SMs)'],\n",
    "    'Total Cores': X_test['Total Cores'],\n",
    "    'L2 Cache Size (KB)': X_test['L2 Cache Size (KB)'],\n",
    "    'y_pred': y_pred,\n",
    "    'y_test': y_test\n",
    "})\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "# Calculate MAPE\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.4f}\")\n",
    "\n",
    "print(results_df.to_csv())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test2: Direct model (sim + gpu, no high variance features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Split data for intermediate model training\n",
    "X_full = full[input_features]\n",
    "y_exec_time = full['exec_time_avg']\n",
    "\n",
    "# Split data for main model training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_exec_time, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the main model with both core and high-variance features\n",
    "main_model = DecisionTreeRegressor(random_state=42)\n",
    "main_model.fit(X_train, y_train)\n",
    "\n",
    "# Make final predictions\n",
    "y_pred = main_model.predict(X_test)\n",
    "\n",
    "# Display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'N': X_test['N'],\n",
    "    'theta': X_test['theta'],\n",
    "    'dt': X_test['dt'],\n",
    "    'Multiprocessors (SMs)': X_test['Multiprocessors (SMs)'],\n",
    "    'Total Cores': X_test['Total Cores'],\n",
    "    'L2 Cache Size (KB)': X_test['L2 Cache Size (KB)'],\n",
    "    'y_pred': y_pred,\n",
    "    'y_test': y_test\n",
    "})\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "# Calculate MAPE\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.4f}\")\n",
    "\n",
    "print(results_df.to_csv())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test3: Direct model (sim + gpu, no high variance features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "# Split data for intermediate model training\n",
    "X_full = full[input_features]\n",
    "y_exec_time = full['exec_time_avg']\n",
    "\n",
    "# Split data for main model training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_exec_time, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the main model with both core and high-variance features\n",
    "main_model = DecisionTreeRegressor(random_state=42)\n",
    "# main_model = LinearRegression() # Linear Kernel\n",
    "main_model.fit(X_train, y_train)\n",
    "\n",
    "# Make final predictions\n",
    "y_pred = main_model.predict(X_test)\n",
    "\n",
    "# Display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'N': X_test['N'],\n",
    "    'theta': X_test['theta'],\n",
    "    'dt': X_test['dt'],\n",
    "    'Multiprocessors (SMs)': X_test['Multiprocessors (SMs)'],\n",
    "    'Total Cores': X_test['Total Cores'],\n",
    "    'L2 Cache Size (KB)': X_test['L2 Cache Size (KB)'],\n",
    "    'y_pred': y_pred,\n",
    "    'y_test': y_test\n",
    "})\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "# Calculate MAPE\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.4f}\")\n",
    "\n",
    "print(results_df.to_csv())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4: Multitask learning output using min, max and avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_full.to_csv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_comp():\n",
    "    # Convert y_test and y_pred to DataFrames for easy plotting\n",
    "    y_test_df = pd.DataFrame(y_test, columns=['exec_time_avg', 'exec_time_max', 'exec_time_min']).reset_index(drop=True)\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=['exec_time_avg_pred', 'exec_time_max_pred', 'exec_time_min_pred'])\n",
    "\n",
    "    # Reset index for test groups to match with y_test\n",
    "    test_groups = groups.iloc[test_index].reset_index(drop=True)\n",
    "\n",
    "    # Generate color mapping for each unique GPU name\n",
    "    unique_gpus = test_groups.unique()\n",
    "    palette = sns.color_palette(\"husl\", len(unique_gpus))\n",
    "    color_map = dict(zip(unique_gpus, palette))\n",
    "\n",
    "    # Map colors for each GPU in the test set\n",
    "    colors = test_groups.map(color_map)\n",
    "\n",
    "    # Plot comparisons for each target variable\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle(\"Comparison of Actual vs Predicted Values (Color-coded by GPU)\")\n",
    "\n",
    "    target_names = ['exec_time_avg', 'exec_time_max', 'exec_time_min']\n",
    "    for i, target in enumerate(target_names):\n",
    "        scatter = axes[i].scatter(y_test_df[target], y_pred_df[f\"{target}_pred\"], c=colors, alpha=0.6)\n",
    "        axes[i].plot([y_test_df[target].min(), y_test_df[target].max()],\n",
    "                    [y_test_df[target].min(), y_test_df[target].max()], 'r--')  # Reference line\n",
    "        axes[i].set_xlabel(\"Actual Values\")\n",
    "        axes[i].set_ylabel(\"Predicted Values\")\n",
    "        axes[i].set_title(f\"{target} (Actual vs Predicted)\")\n",
    "        axes[i].grid(True)\n",
    "\n",
    "    # Create a legend for GPU names\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color_map[gpu], markersize=10) for gpu in unique_gpus]\n",
    "    labels = unique_gpus\n",
    "    fig.legend(handles, labels, title=\"GPU Name\", loc=\"upper right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import pandas as pd\n",
    "\n",
    "# Define input and target variables\n",
    "X_full = full[selected_features]\n",
    "\n",
    "a= ['N','theta','dt',\n",
    "#  'Compute Capability',\n",
    "#  'Total Memory (MB)',\n",
    " 'Multiprocessors (SMs)',\n",
    " 'Max Threads Per SM',\n",
    "#  'Total Cores',\n",
    "#  'Warp Size',\n",
    "#  'Max Threads Per Block',\n",
    "#  'Max Blocks Per SM',\n",
    "#  'Shared Memory Per Block (KB)',\n",
    "#  'Shared Memory Per SM (KB)',\n",
    "#  'Registers Per Block',\n",
    "#  'Registers Per SM',\n",
    "#  'L1 Cache Size (KB)',\n",
    " 'L2 Cache Size (KB)',\n",
    "#  'Memory Bus Width (bits)',\n",
    "#  'Memory Bandwidth (GB/s)',\n",
    "#  'Clock Rate (MHz)',\n",
    " 'Warps Per SM',\n",
    " 'Blocks Per SM',\n",
    "#  'Half Precision FLOP/s',\n",
    "#  'Single Precision FLOP/s',\n",
    "#  'Double Precision FLOP/s',\n",
    "#  'Concurrent Kernels',\n",
    "#  'Threads Per Warp',\n",
    "#  'Global Memory Bandwidth (GB/s)',\n",
    "#  'Global Memory Size (MB)',\n",
    "#  'L2 Cache Size',\n",
    "#  'Memcpy Engines'\n",
    " ]\n",
    "\n",
    "y_full = full[['exec_time_avg', 'exec_time_max', 'exec_time_min']]  # Multi-output target\n",
    "\n",
    "# Split data for main model training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the main model with MultiOutputRegressor\n",
    "base_model = DecisionTreeRegressor(random_state=42)\n",
    "main_model = MultiOutputRegressor(base_model)\n",
    "main_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for each target (exec_time_avg, exec_time_max, exec_time_min)\n",
    "y_pred = main_model.predict(X_test)\n",
    "\n",
    "# Convert y_test and y_pred to DataFrames for easy comparison\n",
    "y_test_df = pd.DataFrame(y_test, columns=['exec_time_avg', 'exec_time_max', 'exec_time_min']).reset_index(drop=True)\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=['exec_time_avg_pred', 'exec_time_max_pred', 'exec_time_min_pred'])\n",
    "\n",
    "# Display the results alongside input features\n",
    "results_df = pd.concat([X_test.reset_index(drop=True), y_test_df, y_pred_df], axis=1)\n",
    "\n",
    "# Calculate MAPE for each output\n",
    "mape_avg = mean_absolute_percentage_error(y_test_df['exec_time_avg'], y_pred_df['exec_time_avg_pred'])\n",
    "mape_max = mean_absolute_percentage_error(y_test_df['exec_time_max'], y_pred_df['exec_time_max_pred'])\n",
    "mape_min = mean_absolute_percentage_error(y_test_df['exec_time_min'], y_pred_df['exec_time_min_pred'])\n",
    "\n",
    "print(f\"Mean Absolute Percentage Error (MAPE) - exec_time_avg: {mape_avg:.4f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE) - exec_time_max: {mape_max:.4f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE) - exec_time_min: {mape_min:.4f}\")\n",
    "\n",
    "# Export results to CSV (optional)\n",
    "# print(results_df.to_csv(index=False))\n",
    "plot_comp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N,theta,dt,Multiprocessors (SMs),Max Threads Per SM,L2 Cache Size (KB),Warps Per SM,Blocks Per SM,\n",
    "\n",
    "\n",
    "main_model.predict([[1000000,0.2,6.25e-05,40,1024,4096,32,16]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = np.log1p(full[input_features].apply(pd.to_numeric, errors='coerce'))  # Use log1p to avoid issues with zero values (log(x+1))\n",
    "\n",
    "# Step 2: Normalization (scaling between 0 and 1)\n",
    "scaler = MinMaxScaler()\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(df_scaled), columns=df_scaled.columns)\n",
    "\n",
    "# print(df_normalized.to_csv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold, cross_val_score\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import make_scorer, mean_absolute_percentage_error, mean_squared_error, mean_absolute_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# input_features = [\n",
    "#  'N',\n",
    "#  'theta',\n",
    "#  'dt',\n",
    "#  'Multiprocessors (SMs)',\n",
    "#  'Max Threads Per SM',\n",
    "# #  'L2 Cache Size (KB)',\n",
    "#  'Warps Per SM',\n",
    "# #  'Blocks Per SM',\n",
    "\n",
    "# #  'Compute Capability',\n",
    "# #  'Total Memory (MB)',\n",
    "# #  'Total Cores',\n",
    "# #  'Warp Size',\n",
    "# #  'Max Threads Per Block',\n",
    "# #  'Max Blocks Per SM',\n",
    "# #  'Shared Memory Per Block (KB)',\n",
    "# #  'Shared Memory Per SM (KB)',\n",
    "# #  'Registers Per Block',\n",
    "# #  'Registers Per SM',\n",
    "# #  'L1 Cache Size (KB)',\n",
    "# #  'Memory Bus Width (bits)',\n",
    "# #  'Memory Bandwidth (GB/s)',\n",
    "# #  'Clock Rate (MHz)',\n",
    "# #  'Half Precision FLOP/s',\n",
    "# #  'Single Precision FLOP/s',\n",
    "# #  'Double Precision FLOP/s',\n",
    "# #  'Concurrent Kernels',\n",
    "# #  'Threads Per Warp',\n",
    "# #  'Global Memory Bandwidth (GB/s)',\n",
    "# #  'Global Memory Size (MB)',\n",
    "# #  'L2 Cache Size',\n",
    "# #  'Memcpy Engines'\n",
    "#  ] #+ selected_features\n",
    "input_features = ['N', 'theta', 'Half Precision FLOP/s', 'Single Precision FLOP/s', 'Double Precision FLOP/s']\n",
    "\n",
    "# input_features = ['N', 'gld_transactions (dev_approximate_gravity)',\n",
    "#  'thread_inst_executed (dev_approximate_gravity)',\n",
    "#  'inst_compute_ld_st (correct_particles)',\n",
    "#  'global_store_requests (correct_particles)']\n",
    "# output_features = ['exec_time_avg', 'exec_time_max', 'exec_time_min']\n",
    "output_features = ['exec_time_avg']\n",
    "\n",
    "df_scaled = np.log1p(full[input_features].apply(pd.to_numeric, errors='coerce'))  # Use log1p to avoid issues with zero values (log(x+1))\n",
    "\n",
    "# Step 2: Normalization (scaling between 0 and 1)\n",
    "scaler = MinMaxScaler()\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(df_scaled), columns=df_scaled.columns)\n",
    "\n",
    "\n",
    "# Define input and target variables\n",
    "# input_features = ['N', 'theta', 'dt', 'Multiprocessors (SMs)', 'Total Cores', 'L2 Cache Size (KB)']\n",
    "# X_full = df_normalized[input_features]\n",
    "sample_full = full #resample(full, n_samples=1000)\n",
    "X_full = sample_full[input_features]\n",
    "y_full = sample_full[output_features] # Multi-output target\n",
    "\n",
    "# y_full = np.log10(full[['exec_time_avg', 'exec_time_max', 'exec_time_min']]) # Multi-output target\n",
    "groups = sample_full['Name']  # Group by GPU name\n",
    "# Apply square root transformation\n",
    "\n",
    "# Initialize the model\n",
    "# base_model = DecisionTreeRegressor(random_state=42) #,max_depth=15,max_features=1,max_leaf_nodes=None, min_samples_leaf=4, min_samples_split=2)\n",
    "# Best MAPE Score: 0.3368 , 0.2495\n",
    "\n",
    "# base_model = SVR(kernel='rbf', C=0.001, epsilon=0.000001)  # You can tune 'C' and 'epsilon' for performance\n",
    "\n",
    "\n",
    "# base_model = LinearRegression()\n",
    "base_model = RandomForestRegressor(random_state = 42, bootstrap=True,max_depth=10,max_features=1)\n",
    "main_model = MultiOutputRegressor(base_model)\n",
    "# main_model = base_model\n",
    "\n",
    "# Setup GroupKFold\n",
    "group_kfold = GroupKFold(n_splits=groups.nunique())  # Define the number of splits\n",
    "\n",
    "# Cross-validation with grouped folds\n",
    "scores = []\n",
    "for fold, (train_index, test_index) in enumerate(group_kfold.split(X_full, y_full, groups=groups), start=1):\n",
    "    \n",
    "    X_train, X_test = X_full.iloc[train_index], X_full.iloc[test_index]\n",
    "    y_train, y_test = y_full.iloc[train_index], y_full.iloc[test_index]\n",
    "    y_train_root = np.power(y_train,(1/1))\n",
    "\n",
    "    # Fit the model on the training set\n",
    "    main_model.fit(X_train, y_train_root)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred_root = main_model.predict(X_test)\n",
    "    y_pred = np.power(y_pred_root,1)\n",
    "    \n",
    "    y_test_df = pd.DataFrame(y_test, columns=output_features)\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=output_features)\n",
    "\n",
    "    # print(y_test_df)\n",
    "    fold_mape = mean_absolute_percentage_error(y_test,y_pred)\n",
    "    mape_avg = mean_absolute_percentage_error(y_test_df['exec_time_avg'], y_pred_df['exec_time_avg'])\n",
    "    # mse_avg = mean_squared_error(y_test_df['exec_time_avg'], y_pred_df['exec_time_avg'])\n",
    "    # mae_avg = mean_absolute_error(y_test_df['exec_time_avg'], y_pred_df['exec_time_avg'])\n",
    "    # mape_min = 0# mean_absolute_percentage_error(y_test_df['exec_time_min'], y_pred_df['exec_time_min'])\n",
    "    # mape_max = 0# mean_absolute_percentage_error(y_test_df['exec_time_max'], y_pred_df['exec_time_max'])\n",
    "    \n",
    "    scores.append(fold_mape)\n",
    "    \n",
    "    # Print the GPU names in the test set and the MAPE for this fold\n",
    "    test_gpus = groups.iloc[test_index].unique()\n",
    "    print(f\"Fold {fold}: Test GPUs = {test_gpus}\")\n",
    "    print(f\"Fold {fold}: MAPE = avg: {mape_avg:.4f}\")\n",
    "    # plot_comp()\n",
    "\n",
    "# Calculate the average MAPE across folds\n",
    "average_mape = np.mean(scores)\n",
    "# print(scores)\n",
    "print(f\"Cross-Validated Mean Absolute Percentage Error (MAPE): {average_mape:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = ['N', 'theta', 'Multiprocessors (SMs)','Half Precision FLOP/s', 'Single Precision FLOP/s', \n",
    "                  'Double Precision FLOP/s']\n",
    "output_features = selected_features[0]\n",
    "\n",
    "X_full = full[input_features]\n",
    "# X_full['N_logN'] = full['N'] * np.log(full['N'])\n",
    "y_full = np.power(full[output_features],1/2)\n",
    "\n",
    "X_train, X_test, y_train, y_test, groups_train, groups_val = train_test_split(\n",
    "    X_full, y_full, groups, test_size=0.2, random_state=42 #, stratify=groups\n",
    ")\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "# intermediate_model = KernelRidge(kernel='rbf', alpha=1.0)\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "intermediate_model = GradientBoostingRegressor(max_depth=3,\n",
    "                                n_estimators=1000,\n",
    "                                learning_rate=0.1,\n",
    "                                random_state=42)\n",
    "\n",
    "intermediate_model.fit(X_train, y_train)\n",
    "y_pred = intermediate_model.predict(X_test)\n",
    "\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "b = []\n",
    "for i in range(10):\n",
    "    ins = [a*10**i,0.4,34,10396,5198,5198] \n",
    "    b.append(best_model.predict([ins])[0])\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5: Linear interpolated model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.query('dt == 0.0625 and N==100000 and `Clock Rate (MHz)` == 1785 and `L2 Cache Size (KB)` == 4096').plot(x='theta', y='exec_time_avg', marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data\n",
    "# Replace with your actual dataset\n",
    "data = full\n",
    "\n",
    "# pd.DataFrame({\n",
    "#     'N': [...],\n",
    "#     'dt': [...],\n",
    "#     'theta': [...],\n",
    "#     'Name': [...],\n",
    "#     'exec_time_avg': [...]\n",
    "# })\n",
    "\n",
    "# Get unique combinations of dt, theta, and Name\n",
    "combinations = data[['dt', 'theta', 'Clock Rate (MHz)','L2 Cache Size (KB)']].drop_duplicates()\n",
    "models = {}\n",
    "\n",
    "for _, row in combinations.iterrows():\n",
    "    dt_val = row['dt']\n",
    "    theta_val = row['theta']\n",
    "    clock_rate_val = row['Clock Rate (MHz)']\n",
    "    L2_cache_val = row['L2 Cache Size (KB)']\n",
    "    # name_val = row['Name']\n",
    "\n",
    "    # Filter the dataset based on the fixed combination\n",
    "    subset = data.query('dt == @dt_val and theta == @theta_val and `Clock Rate (MHz)` == @clock_rate_val and `L2 Cache Size (KB)` == @L2_cache_val')\n",
    "\n",
    "    # Fit a linear regression model for this subset\n",
    "    if len(subset) > 1:  # Make sure there's enough data to fit\n",
    "        model = ElasticNet()\n",
    "        X = subset[['N']]\n",
    "        y = subset['exec_time_avg']\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # Store the model with a key identifying the fixed values\n",
    "        models[(dt_val, theta_val, clock_rate_val,L2_cache_val)] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_exec_time_avg(N, dt, theta, clock_rate, L2_cache):\n",
    "    # Find the corresponding model for the given dt, theta, and Name\n",
    "    model_key = (dt, theta, clock_rate, L2_cache)\n",
    "    model = models.get(model_key)\n",
    "    \n",
    "    if model:\n",
    "        # Predict using the stored model for this combination\n",
    "        return model.predict([[N]])[0]\n",
    "    else:\n",
    "        print(\"No model found for this combination.\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "predicted_time = predict_exec_time_avg(N=100, dt=0.0625, theta=0.8, clock_rate= 1785, L2_cache = 4096)\n",
    "print(f\"Predicted execution time: {predicted_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.query('N==100 and dt == 0.0625 and theta==0.8 and `Clock Rate (MHz)` == 1785 and `L2 Cache Size (KB)` == 4096')[\"exec_time_avg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1349,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data\n",
    "# Replace with your actual dataset\n",
    "data = full\n",
    "\n",
    "# Get unique combinations of dt, theta, Clock Rate, and L2 Cache Size\n",
    "combinations = data[['dt', 'theta', 'Clock Rate (MHz)', 'L2 Cache Size (KB)']].drop_duplicates()\n",
    "models = {}\n",
    "combination_keys = []\n",
    "\n",
    "# Fit linear models for each unique combination of (dt, theta, Clock Rate, L2 Cache Size)\n",
    "for _, row in combinations.iterrows():\n",
    "    dt_val = row['dt']\n",
    "    theta_val = row['theta']\n",
    "    clock_rate_val = row['Clock Rate (MHz)']\n",
    "    L2_cache_val = row['L2 Cache Size (KB)']\n",
    "\n",
    "    # Filter the dataset based on the fixed combination\n",
    "    subset = data.query('dt == @dt_val and theta == @theta_val and `Clock Rate (MHz)` == @clock_rate_val and `L2 Cache Size (KB)` == @L2_cache_val')\n",
    "\n",
    "    # Fit a linear regression model for this subset\n",
    "    if len(subset) > 1:  # Ensure there's enough data to fit\n",
    "        # model = ElasticNet()\n",
    "        model =  make_pipeline(PolynomialFeatures(degree=3), LinearRegression())\n",
    "\n",
    "        X = subset[['N']]\n",
    "        y = subset['exec_time_avg']\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # Store the model and corresponding feature combination for interpolation\n",
    "        models[(dt_val, theta_val, clock_rate_val, L2_cache_val)] = model\n",
    "        combination_keys.append((dt_val, theta_val, clock_rate_val, L2_cache_val))\n",
    "\n",
    "# # Save models to disk (optional) using joblib or pickle\n",
    "# import joblib\n",
    "# joblib.dump(models, 'saved_models.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load the saved models\n",
    "# models = joblib.load('saved_models.pkl')\n",
    "\n",
    "def query_prediction(N, dt, theta, clock_rate, L2_cache):\n",
    "    # Find the closest match to the requested combination\n",
    "    combination_key = (dt, theta, clock_rate, L2_cache)\n",
    "    # If the combination exists in the saved models\n",
    "    if combination_key in models:\n",
    "        model = models[combination_key]\n",
    "        return model.predict([[N]])[0]  # Predict using the model for this combination\n",
    "    else:\n",
    "        print(\"Combination not found. Using interpolation.\")\n",
    "        \n",
    "        # Perform interpolation using griddata if no exact match is found\n",
    "        combination_keys = np.array(list(models.keys()))  # Get known combinations\n",
    "        known_predictions = []\n",
    "        \n",
    "        # Get predictions for all known combinations\n",
    "        for comb_key in combination_keys:\n",
    "            model = models[tuple(comb_key)]\n",
    "            known_predictions.append(model.predict([[N]])[0])\n",
    "        \n",
    "        print(combination_keys)\n",
    "        # Interpolate predictions for the new combination\n",
    "        interpolated_value = griddata(combination_keys, known_predictions, np.array([[dt, theta, clock_rate,L2_cache]]), method='linear')\n",
    "        print(interpolated_value)\n",
    "        return interpolated_value[0] if interpolated_value.size > 0 else None\n",
    "\n",
    "# Example usage\n",
    "predicted_time = query_prediction(N=10000000, dt=0.0625, theta=0.8, clock_rate= 1785, L2_cache = 1096)\n",
    "print(f\"Predicted execution time: {predicted_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.query('N==10000000 and dt == 0.0625 and theta==0.2 and `Clock Rate (MHz)` == 1785 and `L2 Cache Size (KB)` == 4096')[\"exec_time_avg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full['Multiprocessors (SMs)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 6: CV tree and svr test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold, train_test_split, GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define input and target variables\n",
    "input_features = ['N', 'theta', 'Multiprocessors (SMs)','Half Precision FLOP/s', 'Single Precision FLOP/s', \n",
    "                  'Double Precision FLOP/s'] # + selected_features\n",
    "\n",
    "# input_features = ['N', 'theta','dt', 'Clock Rate (MHz)', 'L2 Cache Size (KB)','Multiprocessors (SMs)','Half Precision FLOP/s', 'Single Precision FLOP/s', 'Double Precision FLOP/s']\n",
    "\n",
    "output_features = ['exec_time_avg']\n",
    "grouping_feature = 'Name'\n",
    "\n",
    "f_full = full\n",
    "# output_features = selected_features\n",
    "# X_full = pd.concat([full[input_features], np.power(full[selected_features[0]],1/10)], axis=1) \n",
    "X_full = f_full[input_features]\n",
    "\n",
    "# X_full = pd.concat([full[input_features], pd.DataFrame(intermediate_model.predict(full[input_features]),columns=[selected_features[0]])], axis=1) \n",
    "# y_full = np.power(full[output_features],1/10)\n",
    "y_full = f_full[output_features]\n",
    "groups = f_full[grouping_feature]  # Group by GPU name\n",
    "\n",
    "# Step 1: Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val, groups_train, groups_val = train_test_split(\n",
    "    X_full, y_full, groups, test_size=0.2, random_state=42, stratify=groups\n",
    ")\n",
    "\n",
    "quantile_transformer = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "\n",
    "# Step 2: Initialize the base model and define the parameter grid\n",
    "base_model_rf = RandomForestRegressor(random_state=42)\n",
    "base_model_svr = SVR()\n",
    "base_model_poly = Pipeline([\n",
    "                    ('poly', PolynomialFeatures()),   # PolynomialFeatures step\n",
    "                    ('ridge', Ridge())                # Ridge regression step\n",
    "                ])\n",
    "base_model_krr = KernelRidge()\n",
    "base_model_gb = GradientBoostingRegressor(random_state=42)\n",
    "base_model_elasticnet = ElasticNet(random_state=42)\n",
    "base_model_mlp = MLPRegressor(random_state=42)\n",
    "base_model_etr = ExtraTreesRegressor(random_state=42)\n",
    "\n",
    "param_distributions_rf = {\n",
    "    'estimator__n_estimators': [1,5,10,25,50, 100, 200],\n",
    "    'estimator__max_depth': [2,5,10, 20, None],\n",
    "    'estimator__min_samples_split': [2, 5, 10],\n",
    "    'estimator__min_samples_leaf': [1,2, 4,6],\n",
    "    'estimator__max_features': [1, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "param_distributions_etr = {\n",
    "    'estimator__n_estimators': [50, 100, 200, 300],        # Number of trees in the forest\n",
    "    'estimator__max_depth': [None, 10, 20, 30],            # Maximum depth of the trees\n",
    "    'estimator__min_samples_split': [2, 5, 10],            # Minimum number of samples required to split an internal node\n",
    "    'estimator__min_samples_leaf': [1, 2, 4],              # Minimum number of samples required to be at a leaf node\n",
    "    'estimator__max_features': ['sqrt', 'log2', None],     # Number of features to consider when looking for the best split\n",
    "    'estimator__bootstrap': [True, False],                 # Whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "param_distributions_svr = {\n",
    "    'estimator__C': [0.1, 1, 10, 100, 1000],  # Regularization parameter\n",
    "    'estimator__epsilon': [0.0001, 0.001, 0.01, 0.1, 0.5],  # Margin of error\n",
    "    'estimator__kernel': ['rbf'],  # Kernel types\n",
    "    #'estimator__degree': [2, 3, 4],  # Only for 'poly' kernel\n",
    "    'estimator__gamma': ['scale', 'auto'],  # Kernel coefficient\n",
    "}\n",
    "\n",
    "param_distributions_poly = {\n",
    "    'poly__degree': [2, 3, 4, 5],                  # Degree of polynomial features\n",
    "    'ridge__alpha': [0.1, 1, 10, 100, 1000],       # Regularization parameter for Ridge\n",
    "    'ridge__fit_intercept': [True, False],         # Whether to fit the intercept term\n",
    "    'ridge__normalize': [True, False],             # Normalize inputs\n",
    "}\n",
    "\n",
    "param_distributions_krr = {\n",
    "    'estimator__alpha': [0.1, 1, 10, 100, 1000],  # Regularization strength\n",
    "    'estimator__kernel': ['rbf', ],  # Kernel types\n",
    "    # 'estimator__degree': [2, 3, 4],  # Degree of polynomial kernel (only used if kernel='polynomial')\n",
    "    'estimator__gamma': [0.01,0.1,1,None],  # Kernel coefficient for 'rbf' and 'sigmoid'\n",
    "}\n",
    "\n",
    "param_distributions_gb = {\n",
    "    'estimator__n_estimators': [50, 100, 200, 500],  # Number of boosting stages\n",
    "    'estimator__learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],  # Step size shrinkage\n",
    "    'estimator__max_depth': [3, 5, 7, 10],  # Maximum depth of each estimator\n",
    "    'estimator__min_samples_split': [2, 5, 10],  # Minimum samples required to split\n",
    "    'estimator__min_samples_leaf': [1, 2, 4],  # Minimum samples required in leaf nodes\n",
    "    'estimator__subsample': [0.6, 0.8, 1.0],  # Fraction of samples used per estimator\n",
    "    'estimator__max_features': [1, 'sqrt', 'log2']  # Number of features to consider at each split\n",
    "}\n",
    "\n",
    "param_distributions_elasticnet = {\n",
    "    'estimator__alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization strength\n",
    "    'estimator__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9, 1.0,],  # Mix between L1 and L2 (lasso and ridge)\n",
    "    'estimator__fit_intercept': [True, False],  # Whether to calculate the intercept\n",
    "    'estimator__max_iter': [100000],  # Whether to calculate the intercept\n",
    "}\n",
    "\n",
    "param_distributions_mlp = {\n",
    "    'estimator__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100), (50, 100, 50)],  # Different hidden layer configurations\n",
    "    'estimator__activation': ['relu', 'tanh', 'logistic'],  # Activation functions\n",
    "    'estimator__solver': ['adam', 'sgd'],  # Optimization algorithms\n",
    "    'estimator__alpha': [0.0001, 0.001, 0.01, 0.1],  # L2 regularization term\n",
    "    'estimator__learning_rate': ['constant', 'adaptive'],  # Learning rate schedule\n",
    "    'estimator__learning_rate_init': [0.001, 0.01, 0.1],  # Initial learning rate\n",
    "    'estimator__max_iter': [200, 300, 500]  # Number of epochs\n",
    "}\n",
    "\n",
    "base_model = base_model_etr\n",
    "param_distributions = param_distributions_etr\n",
    "#{'model__estimator__max_depth': [None], 'model__estimator__max_features': [1], 'model__estimator__min_samples_leaf': [1], 'model__estimator__min_samples_split': [2], 'model__estimator__n_estimators': [10]}\n",
    " #param_distributions_gb\n",
    "\n",
    "# Step 3: Set up MultiOutputRegressor with RandomizedSearchCV on training set\n",
    "main_model = MultiOutputRegressor(base_model)\n",
    "# int_mod = Pipeline([\n",
    "#     ('quantile_transform', quantile_transformer),  # Step to transform target\n",
    "#     ('model', base_model)\n",
    "# ])\n",
    "\n",
    "# main_model = TransformedTargetRegressor(\n",
    "#     # regressor=MultiOutputRegressor(base_model),\n",
    "#     regressor=int_mod,\n",
    "#     transformer=quantile_transformer\n",
    "# )\n",
    "\n",
    "\n",
    "group_kfold = GroupKFold(n_splits=min(5, groups_train.nunique()))  # Number of splits limited to unique group count\n",
    "\n",
    "random_search = GridSearchCV(\n",
    "    estimator=main_model,\n",
    "    param_grid=param_distributions,\n",
    "    # n_iter=100,  # Adjust based on computational resources\n",
    "    cv=group_kfold.split(X_train, y_train, groups=groups_train),\n",
    "    scoring='neg_mean_absolute_percentage_error',\n",
    "    # random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Step 4: Fit the RandomizedSearchCV on the training data only\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the corresponding MAPE score\n",
    "best_mape = -random_search.best_score_  # Convert to positive MAPE\n",
    "print(f\"Best MAPE Score: {best_mape:.4f}\")\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "\n",
    "# Step 5: Use the best parameters to fit a final model and evaluate on validation set\n",
    "best_model = random_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "# Store the MAPE scores for each fold along with group names\n",
    "fold_scores = []\n",
    "group_names = []\n",
    "\n",
    "# Loop through each fold, get the groups and MAPE score\n",
    "for train_index, test_index in group_kfold.split(X_train, y_train, groups=groups_train):\n",
    "    # Get group names for the test set in this fold\n",
    "    test_group_names = groups_train.iloc[test_index].unique()\n",
    "    group_names.append(test_group_names)\n",
    "\n",
    "    # Train on the best model and compute the score for this fold\n",
    "    best_model.fit(X_train.iloc[train_index], y_train.iloc[train_index])\n",
    "    y_pred = best_model.predict(X_train.iloc[test_index])\n",
    "    fold_mape = mean_absolute_percentage_error(y_train.iloc[test_index], y_pred)\n",
    "\n",
    "    # Store the score\n",
    "    fold_scores.append(fold_mape)\n",
    "\n",
    "# Print results with group names\n",
    "for fold_idx, (score, group) in enumerate(zip(fold_scores, group_names), start=1):\n",
    "    print(f\"Fold {fold_idx} (Groups: {group}): MAPE = {score:.4f}\")\n",
    "\n",
    "# Calculate and print the mean MAPE across all folds\n",
    "mean_mape = np.mean(fold_scores)\n",
    "print(f\"Mean MAPE across all folds: {mean_mape:.4f}\")\n",
    "\n",
    "\n",
    "X_val_truth = X_val\n",
    "# X_val_truth = pd.concat([X_val[input_features].reset_index(drop=True), pd.DataFrame(intermediate_model.predict(X_val[input_features]),columns=[selected_features[0]]).reset_index(drop=True)],axis=1)\n",
    "\n",
    "y_val_pred = best_model.predict(X_val_truth)\n",
    "\n",
    "# Calculate and display the MAPE for the validation set\n",
    "mape_val = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "print(f\"Final Validation MAPE with Best Parameters: {mape_val:.4f}\")\n",
    "# print(\"Best Parameters from Randomized Search:\", random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 7: CV whole model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.query('dt == 0.625 and theta == 0.2 and N == 1000000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_full = full # .query('dt <= 625') # filter dt more than 65 theta == 0.8  and Name == \"NVIDIA GeForce GTX 1060 6GB\" and theta <= 0.4 \n",
    "f_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold, train_test_split, GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define input and target variables\n",
    "# input_features = ['N', 'theta', 'dt', 'Multiprocessors (SMs)','Half Precision FLOP/s', 'Single Precision FLOP/s', \n",
    "#                   'Double Precision FLOP/s'] # + selected_features\n",
    "\n",
    "input_features = ['N','theta']\n",
    "\n",
    "# input_features = ['N', 'theta','dt', 'Clock Rate (MHz)', 'L2 Cache Size (KB)','Multiprocessors (SMs)','Half Precision FLOP/s', 'Single Precision FLOP/s', 'Double Precision FLOP/s']\n",
    "\n",
    "output_features = ['exec_time_avg']\n",
    "grouping_feature = 'Name'\n",
    "\n",
    "# f_full = full.query('dt == 0.625 and theta <= 0.8 and N == 1000000') # filter dt more than 65\n",
    "\n",
    "# output_features = selected_features\n",
    "# X_full = pd.concat([full[input_features], np.power(full[selected_features[0]],1/10)], axis=1) \n",
    "X_full = f_full[input_features]\n",
    "\n",
    "# X_full = pd.concat([full[input_features], pd.DataFrame(intermediate_model.predict(full[input_features]),columns=[selected_features[0]])], axis=1) \n",
    "# y_full = np.power(full[output_features],1/10)\n",
    "y_full = f_full[output_features]\n",
    "# groups = f_full[grouping_feature]  # Group by GPU name\n",
    "\n",
    "# Step 1: Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, random_state=42 #,  stratify=groups\n",
    ")\n",
    "\n",
    "quantile_transformer = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "\n",
    "# Step 2: Initialize the base model and define the parameter grid\n",
    "base_model_rf = RandomForestRegressor(random_state=42)\n",
    "base_model_svr = SVR()\n",
    "base_model_poly = Pipeline([\n",
    "                    ('poly', PolynomialFeatures()),   # PolynomialFeatures step\n",
    "                    ('ridge', Ridge())                # Ridge regression step\n",
    "                ])\n",
    "base_model_krr = KernelRidge()\n",
    "base_model_gb = GradientBoostingRegressor(random_state=42)\n",
    "base_model_elasticnet = ElasticNet(random_state=42)\n",
    "base_model_mlp = MLPRegressor(random_state=42)\n",
    "base_model_etr = ExtraTreesRegressor(random_state=42)\n",
    "\n",
    "param_distributions_rf = {\n",
    "    'estimator__n_estimators': [1,5,10,25,50, 100, 200],\n",
    "    'estimator__max_depth': [2,5,10, 20, None],\n",
    "    'estimator__min_samples_split': [2, 5, 10],\n",
    "    'estimator__min_samples_leaf': [1,2, 4,6],\n",
    "    'estimator__max_features': [1, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "param_distributions_etr = {\n",
    "    'estimator__n_estimators': [50, 100, 200, 300],        # Number of trees in the forest\n",
    "    'estimator__max_depth': [None, 10, 20, 30],            # Maximum depth of the trees\n",
    "    'estimator__min_samples_split': [2, 5, 10],            # Minimum number of samples required to split an internal node\n",
    "    'estimator__min_samples_leaf': [1, 2, 4],              # Minimum number of samples required to be at a leaf node\n",
    "    'estimator__max_features': ['sqrt', 'log2', None],     # Number of features to consider when looking for the best split\n",
    "    'estimator__bootstrap': [True, False],                 # Whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "param_distributions_svr = {\n",
    "    'estimator__C': [0.1, 1, 10, 100, 1000],  # Regularization parameter\n",
    "    'estimator__epsilon': [0.0001, 0.001, 0.01, 0.1, 0.5],  # Margin of error\n",
    "    'estimator__kernel': ['rbf'],  # Kernel types\n",
    "    #'estimator__degree': [2, 3, 4],  # Only for 'poly' kernel\n",
    "    'estimator__gamma': ['scale', 'auto'],  # Kernel coefficient\n",
    "}\n",
    "\n",
    "param_distributions_poly = {\n",
    "    'poly__degree': [2, 3, 4, 5],                  # Degree of polynomial features\n",
    "    'ridge__alpha': [0.1, 1, 10, 100, 1000],       # Regularization parameter for Ridge\n",
    "    'ridge__fit_intercept': [True, False],         # Whether to fit the intercept term\n",
    "    'ridge__normalize': [True, False],             # Normalize inputs\n",
    "}\n",
    "\n",
    "param_distributions_krr = {\n",
    "    'estimator__alpha': [0.1, 1, 10, 100, 1000],  # Regularization strength\n",
    "    'estimator__kernel': ['rbf', ],  # Kernel types\n",
    "    # 'estimator__degree': [2, 3, 4],  # Degree of polynomial kernel (only used if kernel='polynomial')\n",
    "    'estimator__gamma': [0.01,0.1,1,None],  # Kernel coefficient for 'rbf' and 'sigmoid'\n",
    "}\n",
    "\n",
    "param_distributions_gb = {\n",
    "    'estimator__n_estimators': [50, 100, 200, 500],  # Number of boosting stages\n",
    "    'estimator__learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],  # Step size shrinkage\n",
    "    'estimator__max_depth': [3, 5, 7, 10],  # Maximum depth of each estimator\n",
    "    'estimator__min_samples_split': [2, 5, 10],  # Minimum samples required to split\n",
    "    'estimator__min_samples_leaf': [1, 2, 4],  # Minimum samples required in leaf nodes\n",
    "    'estimator__subsample': [0.6, 0.8, 1.0],  # Fraction of samples used per estimator\n",
    "    'estimator__max_features': [1, 'sqrt', 'log2']  # Number of features to consider at each split\n",
    "}\n",
    "\n",
    "param_distributions_elasticnet = {\n",
    "    'model__alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization strength\n",
    "    'model__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9, 1.0,],  # Mix between L1 and L2 (lasso and ridge)\n",
    "    'model__fit_intercept': [True, False],  # Whether to calculate the intercept\n",
    "    'model__max_iter': [100000],  # Whether to calculate the intercept\n",
    "}\n",
    "\n",
    "param_distributions_mlp = {\n",
    "    'estimator__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100), (50, 100, 50)],  # Different hidden layer configurations\n",
    "    'estimator__activation': ['relu', 'tanh', 'logistic'],  # Activation functions\n",
    "    'estimator__solver': ['adam', 'sgd'],  # Optimization algorithms\n",
    "    'estimator__alpha': [0.0001, 0.001, 0.01, 0.1],  # L2 regularization term\n",
    "    'estimator__learning_rate': ['constant', 'adaptive'],  # Learning rate schedule\n",
    "    'estimator__learning_rate_init': [0.001, 0.01, 0.1],  # Initial learning rate\n",
    "    'estimator__max_iter': [200, 300, 500]  # Number of epochs\n",
    "}\n",
    "\n",
    "\n",
    "# PARAMS\n",
    "\n",
    "base_model = base_model_elasticnet\n",
    "param_distributions = param_distributions_elasticnet\n",
    "\n",
    "\n",
    "#{'model__estimator__max_depth': [None], 'model__estimator__max_features': [1], 'model__estimator__min_samples_leaf': [1], 'model__estimator__min_samples_split': [2], 'model__estimator__n_estimators': [10]}\n",
    " #param_distributions_gb\n",
    "\n",
    "# Step 3: Set up MultiOutputRegressor with RandomizedSearchCV on training set\n",
    "# main_model = MultiOutputRegressor(base_model)\n",
    "\n",
    "# main_model = MultiOutputRegressor(base_model)\n",
    "\n",
    "main_model = Pipeline([\n",
    "    ('quantile_transform', quantile_transformer),  # Step to transform target\n",
    "    ('model', base_model)\n",
    "])\n",
    "\n",
    "# main_model = TransformedTargetRegressor(\n",
    "#     # regressor=MultiOutputRegressor(base_model),\n",
    "#     regressor=int_mod,\n",
    "#     transformer=quantile_transformer\n",
    "# )\n",
    "\n",
    "\n",
    "# group_kfold = GroupKFold(n_splits=min(5, groups_train.nunique()))  # Number of splits limited to unique group count\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=main_model,\n",
    "    # param_grid=param_distributions,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,  # Adjust based on computational resources\n",
    "    # cv=group_kfold.split(X_train, y_train, groups=groups_train),\n",
    "    scoring='neg_mean_absolute_percentage_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Step 4: Fit the RandomizedSearchCV on the training data only\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the corresponding MAPE score\n",
    "best_mape = -random_search.best_score_  # Convert to positive MAPE\n",
    "print(f\"Best MAPE Score: {best_mape:.4f}\")\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "\n",
    "# Step 5: Use the best parameters to fit a final model and evaluate on validation set\n",
    "best_model = random_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "# Store the MAPE scores for each fold along with group names\n",
    "fold_scores = []\n",
    "# group_names = []\n",
    "\n",
    "X_val_truth = X_val\n",
    "# X_val_truth = pd.concat([X_val[input_features].reset_index(drop=True), pd.DataFrame(intermediate_model.predict(X_val[input_features]),columns=[selected_features[0]]).reset_index(drop=True)],axis=1)\n",
    "\n",
    "y_val_pred = best_model.predict(X_val_truth)\n",
    "\n",
    "# Calculate and display the MAPE for the validation set\n",
    "mape_val = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "print(f\"Final Validation MAPE with Best Parameters: {mape_val:.4f}\")\n",
    "# print(\"Best Parameters from Randomized Search:\", random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 8: Tester transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of normalizing input and output variables for regression.\n",
    "from numpy import mean\n",
    "from numpy import absolute\n",
    "from numpy import loadtxt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler,PowerTransformer,FunctionTransformer,QuantileTransformer,StandardScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# split into inputs and outputs\n",
    "X, y = f_full[['N', 'theta', 'dt', 'Clock Rate (MHz)', 'L2 Cache Size (KB)','Multiprocessors (SMs)' , 'Half Precision FLOP/s', 'Single Precision FLOP/s', 'Double Precision FLOP/s']], f_full['exec_time_avg']\n",
    "# prepare the model with input scaling\n",
    "pipeline = Pipeline(steps=[('normalize', PowerTransformer()), ('model', RandomForestRegressor(min_samples_split=10,min_samples_leaf=6,n_estimators=100,max_depth=20,max_features=0.2))]) # CHECK: SVR(kernel='rbf',C=0.1,epsilon=0.0001) HuberRegressor(alpha=0.01,epsilon=1.5)\n",
    "# prepare the model with target scaling\n",
    "model = TransformedTargetRegressor(regressor=pipeline, transformer=QuantileTransformer())\n",
    "# evaluate model\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# convert scores to positive\n",
    "scores = absolute(scores)\n",
    "# summarize the result\n",
    "s_mean = mean(scores)\n",
    "print('Mean MAE: %.3f' % (s_mean))\n",
    "\n",
    "cv2 = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_percentage_error', cv=cv2, n_jobs=-1)\n",
    "# convert scores to positive\n",
    "scores = absolute(scores)\n",
    "# summarize the result\n",
    "s_mean = mean(scores)\n",
    "print('Mean MAPE: %.3f' % (s_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_full.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "x = f_full[['Clock Rate (MHz)']]\n",
    "print(x.describe())\n",
    "# [Issue #1]\n",
    "# RuntimeWarning: overflow encountered in multiply from `x_trans_var = x_trans.var()`\n",
    "s = PowerTransformer(method=\"yeo-johnson\", standardize=False).fit_transform(x)\n",
    "s = StandardScaler()\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 9: Second attempt cv preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import mean, absolute\n",
    "# from itertools import combinations\n",
    "# from sklearn.model_selection import KFold,GroupKFold, GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.linear_model import HuberRegressor\n",
    "# from sklearn.preprocessing import PowerTransformer, QuantileTransformer, MinMaxScaler, FunctionTransformer\n",
    "# from sklearn.compose import TransformedTargetRegressor\n",
    "# from sklearn.metrics import make_scorer, mean_absolute_error, mean_absolute_percentage_error\n",
    "# import pandas as pd\n",
    "\n",
    "# # Split the dataset\n",
    "# X_full = f_full[['N', 'theta', 'dt', 'Clock Rate (MHz)', 'L2 Cache Size (KB)','Multiprocessors (SMs)', 'Half Precision FLOP/s', 'Single Precision FLOP/s', 'Double Precision FLOP/s']] #'Multiprocessors (SMs)','Half Precision FLOP/s', 'Single Precision FLOP/s',                   'Double Precision FLOP/s']]\n",
    "# y = f_full['exec_time_avg']\n",
    "# groups = f_full['Name']  # The column used for grouping\n",
    "\n",
    "# # Generate all feature subsets\n",
    "# feature_subsets = []\n",
    "# for r in range(1, len(X_full.columns) + 1):  # r = subset size (1 to len(columns))\n",
    "#     feature_subsets.extend(combinations(X_full.columns, r))\n",
    "\n",
    "# # feature_subsets = [['N', 'theta', 'dt'], ['N', 'theta', 'dt', 'Clock Rate (MHz)', 'L2 Cache Size (KB)','Multiprocessors (SMs)' , 'Half Precision FLOP/s', 'Single Precision FLOP/s', 'Double Precision FLOP/s']]\n",
    "\n",
    "# # Define hyperparameters for the model\n",
    "# model_params = {\n",
    "#     'regressor__model__epsilon': [1.35, 1.5, 1.75],\n",
    "#     'regressor__model__alpha': [0.0001, 0.001, 0.01],\n",
    "# }\n",
    "\n",
    "# # Custom GridSearchCV to handle feature subsets\n",
    "# results = []\n",
    "\n",
    "# # cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "# # cv = GroupKFold(n_splits=5)  # Adjust the number of splits if needed\n",
    "\n",
    "# scorer_mae = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "# scorer_mape = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
    "\n",
    "# ## Split CV\n",
    "# cv_split = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# ## GPU group CV\n",
    "# cv_GPU = GroupKFold(n_splits=5)  # Adjust the number of splits if needed\n",
    "\n",
    "# # Iterate over feature subsets\n",
    "# for features in feature_subsets:\n",
    "#     # Subset the input data\n",
    "#     X = X_full[list(features)]\n",
    "    \n",
    "#     # Define the model (with pipelines if needed)\n",
    "#     pipeline = Pipeline(steps=[\n",
    "#         ('normalize', PowerTransformer()), \n",
    "#         ('model', HuberRegressor())\n",
    "#     ])\n",
    "    \n",
    "#     # Use a TransformedTargetRegressor if output transformation is needed\n",
    "#     model = TransformedTargetRegressor(\n",
    "#         regressor=pipeline, \n",
    "#         transformer=QuantileTransformer()\n",
    "#     )\n",
    "    \n",
    "    \n",
    "#     # Create a GridSearchCV object with GroupKFold\n",
    "#     grid_search = GridSearchCV(\n",
    "#         estimator=model,\n",
    "#         param_grid=model_params,  # Parameter grid to search\n",
    "#         scoring={'MAE': scorer_mae, 'MAPE': scorer_mape},  # Multiple metrics\n",
    "#         refit='MAPE',  # Optimize based on MAE\n",
    "\n",
    "#         ## Split CV\n",
    "#         # cv=cv_split,\n",
    "\n",
    "#         ## GPU Group CV\n",
    "#         cv=cv_GPU.split(X, y, groups), \n",
    "        \n",
    "#         n_jobs=-1  # Use all available cores\n",
    "#     )\n",
    "    \n",
    "#     # Fit the GridSearchCV object\n",
    "#     grid_search.fit(X, y)\n",
    "    \n",
    "#     # Store results\n",
    "#     results.append({\n",
    "#         'features': features,\n",
    "#         'best_params': grid_search.best_params_,\n",
    "#         'best_mae': -grid_search.best_score_,  # Convert back to positive since MAE is negative\n",
    "#         'best_mape': -grid_search.cv_results_['mean_test_MAPE'][grid_search.best_index_],  # Same for MAPE\n",
    "#     })\n",
    "\n",
    "# # Find the best result\n",
    "# best_result = min(results, key=lambda x: x['best_mae'])\n",
    " \n",
    "# # Display results\n",
    "# print(\"Best feature subset:\", best_result['features'])\n",
    "# print(\"Best parameters:\", best_result['best_params'])\n",
    "# print(\"Best MAE:\", best_result['best_mae'])\n",
    "# print(\"Best MAPE:\", best_result['best_mape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapped Cross Validation Grid Search Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean, absolute\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import KFold, GroupKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, mean_absolute_percentage_error\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "\n",
    "def run_feature_subset_cv(\n",
    "    f_full, \n",
    "    input_features, \n",
    "    target_feature, \n",
    "    model, \n",
    "    model_params, \n",
    "    extra_features=[], \n",
    "    use_gpu_cv=False,\n",
    "    use_feature_subsets=True, \n",
    "    refit_error='MAPE', #MAPE OR MAE\n",
    "    output_folder=\"CV_results\"\n",
    "):\n",
    "    \n",
    "    # Prepare input and target\n",
    "    X_full = f_full[input_features+extra_features]\n",
    "    y = f_full[target_feature]\n",
    "    groups = f_full['Name'] if use_gpu_cv else None  # Use 'Name' for GroupKFold if applicable\n",
    "\n",
    "    # Generate feature subsets or use all features\n",
    "    if use_feature_subsets:\n",
    "        feature_subsets = []\n",
    "        for r in range(1, len(X_full[input_features].columns) + 1):  # r = subset size (1 to len(columns))\n",
    "            feature_subsets.extend(combinations(X_full[input_features].columns, r))\n",
    "    else:\n",
    "        feature_subsets = [input_features]  # Use all input features as one subset\n",
    "    \n",
    "    # Define cross-validation strategy\n",
    "    cv = GroupKFold(n_splits=5) if use_gpu_cv else KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "    # Define scoring metrics\n",
    "    scorer_mae = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "    scorer_mape = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
    "\n",
    "    # Store results\n",
    "    results = []\n",
    "\n",
    "    print(f\"Testing {len(feature_subsets)} feature subset(s)...\")\n",
    "    with tqdm(total=len(feature_subsets), desc=\"Progress\", unit=\"subset\") as pbar, \\\n",
    "         tqdm(total=len(feature_subsets), desc=\"Custom Text\", unit=\"update\") as text_pbar:\n",
    "                # Iterate over feature subsets\n",
    "        for features in feature_subsets:\n",
    "            # Subset the input data\n",
    "            X = X_full[list(features)+extra_features]\n",
    "\n",
    "            # Create GridSearchCV object\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=model,\n",
    "                param_grid=model_params,\n",
    "                scoring={'MAE': scorer_mae, 'MAPE': scorer_mape},\n",
    "                refit=refit_error,  # Optimize based on MAPE\n",
    "                cv=cv.split(X, y, groups) if use_gpu_cv else cv,\n",
    "                verbose=0,\n",
    "                n_jobs=-1  # Use all available cores\n",
    "            )\n",
    "\n",
    "            # Fit GridSearchCV\n",
    "            grid_search.fit(X, y)\n",
    "\n",
    "            # Append results\n",
    "            results.append({\n",
    "                'features': list(features)+extra_features,\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'best_mae': -grid_search.cv_results_['mean_test_MAE'][grid_search.best_index_],\n",
    "                'best_mape': -grid_search.cv_results_['mean_test_MAPE'][grid_search.best_index_]\n",
    "            })\n",
    "            \n",
    "            pbar.set_postfix({\"current_subset\": str(list(features)+extra_features), \"current_mae\": round(results[-1]['best_mae'], 3),\"current_mape\": round(results[-1]['best_mape'], 3)})\n",
    "            text_pbar.set_description_str(\n",
    "                f\"> best_mae:{round(min(results, key=lambda x: x['best_mae'])['best_mae'], 3)}\\n\"+\n",
    "                f\"> best_mape:{round(min(results, key=lambda x: x['best_mape'])['best_mape'], 3)}\\n\"+\n",
    "                f\"> best_params:{min(results, key=lambda x: x['best_mape'])['best_params']}\\n\"+\n",
    "                f\"> best_features:{min(results, key=lambda x: x['best_mape'])['features']}\")\n",
    "            pbar.update(1)\n",
    "            text_pbar.update(1)\n",
    "\n",
    "    # Find the best result\n",
    "    best_result = min(results, key=lambda x: x['best_mape'])\n",
    "    \n",
    "    # Extract the model name from the pipeline\n",
    "    model_name = model.regressor.steps[-1][1].__class__.__name__\n",
    "\n",
    "    # Determine CV type for the filename\n",
    "    cv_type = \"GPU_CV\" if use_gpu_cv else \"Random_CV\"\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Construct the base filename\n",
    "    base_filename = f\"{model_name}_{cv_type}_results\"\n",
    "    output_path = os.path.join(output_folder, f\"{base_filename}.json\")\n",
    "\n",
    "    # If a file with the same name exists, append a number\n",
    "    counter = 1\n",
    "    while os.path.exists(output_path):\n",
    "        output_path = os.path.join(output_folder, f\"{base_filename}_{counter}.json\")\n",
    "        counter += 1\n",
    "\n",
    "    # Include best result in the JSON data\n",
    "    output_data = {\n",
    "        \"all_results\": results,\n",
    "        \"best_result\": best_result\n",
    "    }\n",
    "\n",
    "    # Write results to a JSON file\n",
    "    with open(output_path, 'w') as json_file:\n",
    "        json.dump(output_data, json_file, indent=4, default=str)  # Use default=str for non-serializable data\n",
    "\n",
    "    print(f\"Results saved to {output_path}\")\n",
    "    \n",
    "    return best_result,results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input + GPU Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber tests\n",
    "* Best Random CV **(MAPE 0.1817):** CV_results/input+GPU/HuberRegressor_Random_CV_results.json \n",
    "* Best GPU CV **(MAPE 0.2463):** CV_results/input+GPU/HuberRegressor_GPU_CV_results.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "# Define dataset and parameters\n",
    "input_features = ['N', 'theta', 'dt', 'Clock Rate (MHz)', 'L2 Cache Size (KB)', \n",
    "                  'Multiprocessors (SMs)', 'Half Precision FLOP/s', 'Single Precision FLOP/s', \n",
    "                  'Double Precision FLOP/s']\n",
    "target_feature = 'exec_time_avg'\n",
    "\n",
    "# Define the model pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('normalize', PowerTransformer()),\n",
    "    ('model', HuberRegressor())\n",
    "])\n",
    "\n",
    "# Wrap with TransformedTargetRegressor\n",
    "model = TransformedTargetRegressor(\n",
    "    regressor=pipeline, \n",
    "    transformer=QuantileTransformer()\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "model_params = {\n",
    "    'regressor__model__epsilon': [1.35, 1.5, 1.75],\n",
    "    'regressor__model__alpha': [0.0001, 0.001, 0.01],\n",
    "}\n",
    "\n",
    "# Run feature subset evaluation using Random CV\n",
    "best_result_hr, res_hr = run_feature_subset_cv(\n",
    "    f_full=f_full,\n",
    "    input_features=input_features,\n",
    "    target_feature=target_feature,\n",
    "    model=model,\n",
    "    model_params=model_params,\n",
    "    use_feature_subsets=True,\n",
    "    use_gpu_cv=False  # Set to False to use regular KFold\n",
    ")\n",
    "\n",
    "# Display the best result\n",
    "print(\"Best feature subset:\", best_result_hr['features'])\n",
    "print(\"Best parameters:\", best_result_hr['best_params'])\n",
    "print(\"Best MAE:\", best_result_hr['best_mae'])\n",
    "print(\"Best MAPE:\", best_result_hr['best_mape'])\n",
    "\n",
    "# Run feature subset evaluation using GPU split CV\n",
    "best_result_hr, res_hr = run_feature_subset_cv(\n",
    "    f_full=f_full,\n",
    "    input_features=input_features,\n",
    "    target_feature=target_feature,\n",
    "    model=model,\n",
    "    model_params=model_params,\n",
    "    use_feature_subsets=True,\n",
    "    use_gpu_cv=True  # Set to False to use regular KFold\n",
    ")\n",
    "\n",
    "# Display the best result\n",
    "print(\"Best feature subset:\", best_result_hr['features'])\n",
    "print(\"Best parameters:\", best_result_hr['best_params'])\n",
    "print(\"Best MAE:\", best_result_hr['best_mae'])\n",
    "print(\"Best MAPE:\", best_result_hr['best_mape'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR tests\n",
    "* Best Random CV **(MAPE 0.0634):** CV_results/input+GPU/SVR_Random_CV_results.json\n",
    "* Best GPU CV **(MAPE 0.2965):** CV_results/input+GPU/SVR_GPU_CV_results.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR, LinearSVR\n",
    "\n",
    "# Define dataset and parameters\n",
    "input_features = ['N', 'theta', 'dt', 'Clock Rate (MHz)', 'L2 Cache Size (KB)', \n",
    "                  'Multiprocessors (SMs)', 'Half Precision FLOP/s', 'Single Precision FLOP/s', \n",
    "                  'Double Precision FLOP/s']\n",
    "target_feature = 'exec_time_avg'\n",
    "\n",
    "# Define the model pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('normalize', PowerTransformer()),\n",
    "    ('model', SVR())\n",
    "])\n",
    "\n",
    "# Wrap with TransformedTargetRegressor\n",
    "model = TransformedTargetRegressor(\n",
    "    regressor=pipeline, \n",
    "    transformer=QuantileTransformer()\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "model_params = {\n",
    "    'regressor__model__C': [0.0001,0.001, 0.01, 0.1, 1, 10, 50,100],\n",
    "    'regressor__model__epsilon': [0.00001,0.0001, 0.001, 0.01, 0.1, 0.5, 1], \n",
    "    'regressor__model__kernel': ['rbf']\n",
    "}\n",
    "# rbf',C=0.1,epsilon=0.0001\n",
    "# Run feature subset evaluation using Random CV\n",
    "best_result_svr, res_svr = run_feature_subset_cv(\n",
    "    f_full=f_full,\n",
    "    input_features=input_features,\n",
    "    target_feature=target_feature,\n",
    "    model=model,\n",
    "    model_params=model_params,\n",
    "    use_feature_subsets=True,\n",
    "    use_gpu_cv=False  # Set to False to use regular KFold\n",
    ")\n",
    "\n",
    "# Display the best result\n",
    "print(\"Best feature subset:\", best_result_svr['features'])\n",
    "print(\"Best parameters:\", best_result_svr['best_params'])\n",
    "print(\"Best MAE:\", best_result_svr['best_mae'])\n",
    "print(\"Best MAPE:\", best_result_svr['best_mape'])\n",
    "\n",
    "# Run feature subset evaluation using GPU split CV\n",
    "best_result_svr, res_svr = run_feature_subset_cv(\n",
    "    f_full=f_full,\n",
    "    input_features=input_features,\n",
    "    target_feature=target_feature,\n",
    "    model=model,\n",
    "    model_params=model_params,\n",
    "    use_feature_subsets=True,\n",
    "    use_gpu_cv=True  # Set to False to use regular KFold\n",
    ")\n",
    "\n",
    "# Display the best result\n",
    "print(\"Best feature subset:\", best_result_svr['features'])\n",
    "print(\"Best parameters:\", best_result_svr['best_params'])\n",
    "print(\"Best MAE:\", best_result_svr['best_mae'])\n",
    "print(\"Best MAPE:\", best_result_svr['best_mape'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT tests\n",
    "* Best Random CV **(MAPE 0.0633):** CV_results/input+GPU/DecisionTreeRegressor_Random_CV_results.json\n",
    "* Best GPU CV **(MAPE 0.2152):** CV_results/input+GPU/DecisionTreeRegressor_GPU_CV_results.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "# Define dataset and parameters\n",
    "input_features = ['N', 'theta', 'dt', 'Clock Rate (MHz)', 'L2 Cache Size (KB)', \n",
    "                  'Multiprocessors (SMs)', 'Half Precision FLOP/s', 'Single Precision FLOP/s', \n",
    "                  'Double Precision FLOP/s']\n",
    "target_feature = 'exec_time_avg'\n",
    "\n",
    "# Define the model pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('normalize', PowerTransformer()),\n",
    "    ('model', DecisionTreeRegressor())\n",
    "])\n",
    "\n",
    "# Wrap with TransformedTargetRegressor\n",
    "model = TransformedTargetRegressor(\n",
    "    regressor=pipeline, \n",
    "    transformer=QuantileTransformer()\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "model_params = {\n",
    "    # 'regressor__model__n_estimators': [1,5,10,25,50, 100],\n",
    "    'regressor__model__max_depth': [2,5,10, 20, None],\n",
    "    'regressor__model__min_samples_split': [2, 5, 10],\n",
    "    'regressor__model__min_samples_leaf': [1, 2, 4],\n",
    "    'regressor__model__max_features': [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Run feature subset evaluation using Random CV\n",
    "best_result_dt, res_dt = run_feature_subset_cv(\n",
    "    f_full=f_full,\n",
    "    input_features=input_features,\n",
    "    target_feature=target_feature,\n",
    "    model=model,\n",
    "    model_params=model_params,\n",
    "    use_feature_subsets=True,\n",
    "    use_gpu_cv=False  # Set to False to use regular KFold\n",
    ")\n",
    "\n",
    "# Display the best result\n",
    "print(\"Best feature subset:\", best_result_dt['features'])\n",
    "print(\"Best parameters:\", best_result_dt['best_params'])\n",
    "print(\"Best MAE:\", best_result_dt['best_mae'])\n",
    "print(\"Best MAPE:\", best_result_dt['best_mape'])\n",
    "\n",
    "# Run feature subset evaluation using GPU split CV\n",
    "best_result_dt, res_dt = run_feature_subset_cv(\n",
    "    f_full=f_full,\n",
    "    input_features=input_features,\n",
    "    target_feature=target_feature,\n",
    "    model=model,\n",
    "    model_params=model_params,\n",
    "    use_feature_subsets=True,\n",
    "    use_gpu_cv=True  # Set to False to use regular KFold\n",
    ")\n",
    "\n",
    "# Display the best result\n",
    "print(\"Best feature subset:\", best_result_dt['features'])\n",
    "print(\"Best parameters:\", best_result_dt['best_params'])\n",
    "print(\"Best MAE:\", best_result_dt['best_mae'])\n",
    "print(\"Best MAPE:\", best_result_dt['best_mape'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERT tests\n",
    "* Best Random CV **(MAPE 0.0633):** CV_results/input+GPU/ExtraTreesRegressor_Random_CV_results.json\n",
    "* Best GPU CV **(MAPE 0.2365):** CV_results/input+GPU/ExtraTreesRegressor_GPU_CV_results.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# Define dataset and parameters\n",
    "input_features = ['N', 'theta', 'dt', 'Clock Rate (MHz)', 'L2 Cache Size (KB)', \n",
    "                  'Multiprocessors (SMs)', 'Half Precision FLOP/s', 'Single Precision FLOP/s', \n",
    "                  'Double Precision FLOP/s']\n",
    "target_feature = 'exec_time_avg'\n",
    "\n",
    "# Define the model pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('normalize', PowerTransformer()),\n",
    "    ('model', ExtraTreesRegressor())\n",
    "])\n",
    "\n",
    "# Wrap with TransformedTargetRegressor\n",
    "model = TransformedTargetRegressor(\n",
    "    regressor=pipeline, \n",
    "    transformer=QuantileTransformer()\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "model_params = {\n",
    "    'regressor__model__n_estimators': [50, 100, 200, 300],        # Number of trees in the forest\n",
    "    'regressor__model__max_depth': [10, 20, 30, None],\n",
    "    'regressor__model__min_samples_split': [2], #, 5, 10\n",
    "    'regressor__model__min_samples_leaf': [1], #, 2, 4\n",
    "    'regressor__model__max_features': [0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "    # 'regressor__model__bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Run feature subset evaluation using Random CV\n",
    "best_result_ert, res_ert = run_feature_subset_cv(\n",
    "    f_full=f_full,\n",
    "    input_features=input_features,\n",
    "    target_feature=target_feature,\n",
    "    model=model,\n",
    "    model_params=model_params,\n",
    "    use_feature_subsets=True,\n",
    "    use_gpu_cv=False  # Set to False to use regular KFold\n",
    ")\n",
    "\n",
    "# Display the best result\n",
    "print(\"Best feature subset:\", best_result_ert['features'])\n",
    "print(\"Best parameters:\", best_result_ert['best_params'])\n",
    "print(\"Best MAE:\", best_result_ert['best_mae'])\n",
    "print(\"Best MAPE:\", best_result_ert['best_mape'])\n",
    "\n",
    "# Run feature subset evaluation using GPU split CV\n",
    "best_result_ert, res_ert = run_feature_subset_cv(\n",
    "    f_full=f_full,\n",
    "    input_features=input_features,\n",
    "    target_feature=target_feature,\n",
    "    model=model,\n",
    "    model_params=model_params,\n",
    "    use_feature_subsets=True,\n",
    "    use_gpu_cv=True  # Set to False to use regular KFold\n",
    ")\n",
    "\n",
    "# Display the best result\n",
    "print(\"Best feature subset:\", best_result_ert['features'])\n",
    "print(\"Best parameters:\", best_result_ert['best_params'])\n",
    "print(\"Best MAE:\", best_result_ert['best_mae'])\n",
    "print(\"Best MAPE:\", best_result_ert['best_mape'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input+GPU+prof Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "# Define dataset and parameters\n",
    "input_features = ['N', 'theta', 'dt', 'Clock Rate (MHz)', 'L2 Cache Size (KB)', \n",
    "                  'Multiprocessors (SMs)', 'Half Precision FLOP/s', 'Single Precision FLOP/s', \n",
    "                  'Double Precision FLOP/s'] + selected_features\n",
    "target_feature = 'exec_time_avg'\n",
    "\n",
    "# Define the model pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('normalize', PowerTransformer()),\n",
    "    ('model', HuberRegressor())\n",
    "])\n",
    "\n",
    "# Wrap with TransformedTargetRegressor\n",
    "model = TransformedTargetRegressor(\n",
    "    regressor=pipeline, \n",
    "    transformer=QuantileTransformer()\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "model_params = {\n",
    "    'regressor__model__epsilon': [1.35, 1.5, 1.75],\n",
    "    'regressor__model__alpha': [0.0001, 0.001, 0.01],\n",
    "}\n",
    "\n",
    "# Run feature subset evaluation using Random CV\n",
    "best_result_hr, res_hr = run_feature_subset_cv(\n",
    "    f_full=f_full,\n",
    "    input_features=input_features,\n",
    "    target_feature=target_feature,\n",
    "    model=model,\n",
    "    model_params=model_params,\n",
    "    output_folder=\"CV_results/input+GPU+prof\",\n",
    "    use_feature_subsets=True,\n",
    "    use_gpu_cv=False  # Set to False to use regular KFold\n",
    ")\n",
    "\n",
    "# Display the best result\n",
    "print(\"Best feature subset:\", best_result_hr['features'])\n",
    "print(\"Best parameters:\", best_result_hr['best_params'])\n",
    "print(\"Best MAE:\", best_result_hr['best_mae'])\n",
    "print(\"Best MAPE:\", best_result_hr['best_mape'])\n",
    "\n",
    "# Run feature subset evaluation using GPU split CV\n",
    "best_result_hr, res_hr = run_feature_subset_cv(\n",
    "    f_full=f_full,\n",
    "    input_features=input_features,\n",
    "    target_feature=target_feature,\n",
    "    model=model,\n",
    "    model_params=model_params,\n",
    "    output_folder=\"CV_results/input+GPU+prof\",\n",
    "    use_feature_subsets=True,\n",
    "    use_gpu_cv=True  # Set to False to use regular KFold\n",
    ")\n",
    "\n",
    "# Display the best result\n",
    "print(\"Best feature subset:\", best_result_hr['features'])\n",
    "print(\"Best parameters:\", best_result_hr['best_params'])\n",
    "print(\"Best MAE:\", best_result_hr['best_mae'])\n",
    "print(\"Best MAPE:\", best_result_hr['best_mape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR, LinearSVR\n",
    "\n",
    "# Define dataset and parameters\n",
    "input_features = ['N', 'theta', 'dt', 'Clock Rate (MHz)', 'L2 Cache Size (KB)', \n",
    "                  'Multiprocessors (SMs)', 'Half Precision FLOP/s', 'Single Precision FLOP/s', \n",
    "                  'Double Precision FLOP/s'] + selected_features\n",
    "target_feature = 'exec_time_avg'\n",
    "\n",
    "# Define the model pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('normalize', PowerTransformer()),\n",
    "    ('model', SVR())\n",
    "])\n",
    "\n",
    "# Wrap with TransformedTargetRegressor\n",
    "model = TransformedTargetRegressor(\n",
    "    regressor=pipeline, \n",
    "    transformer=QuantileTransformer()\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "model_params = {\n",
    "    'regressor__model__C': [0.0001,0.001, 0.01, 0.1, 1, 10, 50,100],\n",
    "    'regressor__model__epsilon': [0.00001,0.0001, 0.001, 0.01, 0.1, 0.5, 1], \n",
    "    'regressor__model__kernel': ['rbf']\n",
    "}\n",
    "# rbf',C=0.1,epsilon=0.0001\n",
    "# Run feature subset evaluation using Random CV\n",
    "best_result_svr, res_svr = run_feature_subset_cv(\n",
    "    f_full=f_full,\n",
    "    input_features=input_features,\n",
    "    target_feature=target_feature,\n",
    "    model=model,\n",
    "    model_params=model_params,\n",
    "    output_folder=\"CV_results/input+GPU+prof\",\n",
    "    use_feature_subsets=True,\n",
    "    use_gpu_cv=False  # Set to False to use regular KFold\n",
    ")\n",
    "\n",
    "# Display the best result\n",
    "print(\"Best feature subset:\", best_result_svr['features'])\n",
    "print(\"Best parameters:\", best_result_svr['best_params'])\n",
    "print(\"Best MAE:\", best_result_svr['best_mae'])\n",
    "print(\"Best MAPE:\", best_result_svr['best_mape'])\n",
    "\n",
    "# Run feature subset evaluation using GPU split CV\n",
    "best_result_svr, res_svr = run_feature_subset_cv(\n",
    "    f_full=f_full,\n",
    "    input_features=input_features,\n",
    "    target_feature=target_feature,\n",
    "    model=model,\n",
    "    model_params=model_params,\n",
    "    output_folder=\"CV_results/input+GPU+prof\",\n",
    "    use_feature_subsets=True,\n",
    "    use_gpu_cv=True  # Set to False to use regular KFold\n",
    ")\n",
    "\n",
    "# Display the best result\n",
    "print(\"Best feature subset:\", best_result_svr['features'])\n",
    "print(\"Best parameters:\", best_result_svr['best_params'])\n",
    "print(\"Best MAE:\", best_result_svr['best_mae'])\n",
    "print(\"Best MAPE:\", best_result_svr['best_mape'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "# Define dataset and parameters\n",
    "input_features = ['N', 'theta', 'dt', 'Clock Rate (MHz)', 'L2 Cache Size (KB)', \n",
    "                  'Multiprocessors (SMs)', 'Half Precision FLOP/s', 'Single Precision FLOP/s', \n",
    "                  'Double Precision FLOP/s'] + selected_features\n",
    "target_feature = 'exec_time_avg'\n",
    "\n",
    "# Define the model pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('normalize', PowerTransformer()),\n",
    "    ('model', DecisionTreeRegressor())\n",
    "])\n",
    "\n",
    "# Wrap with TransformedTargetRegressor\n",
    "model = TransformedTargetRegressor(\n",
    "    regressor=pipeline, \n",
    "    transformer=QuantileTransformer()\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "model_params = {\n",
    "    # 'regressor__model__n_estimators': [1,5,10,25,50, 100],\n",
    "    'regressor__model__max_depth': [2,5,10, 20, None],\n",
    "    'regressor__model__min_samples_split': [2, 5, 10],\n",
    "    'regressor__model__min_samples_leaf': [1, 2, 4],\n",
    "    'regressor__model__max_features': [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Run feature subset evaluation using Random CV\n",
    "best_result_dt, res_dt = run_feature_subset_cv(\n",
    "    f_full=f_full,\n",
    "    input_features=input_features,\n",
    "    target_feature=target_feature,\n",
    "    model=model,\n",
    "    model_params=model_params,\n",
    "    output_folder=\"CV_results/input+GPU+prof\",\n",
    "    use_feature_subsets=True,\n",
    "    use_gpu_cv=False  # Set to False to use regular KFold\n",
    ")\n",
    "\n",
    "# Display the best result\n",
    "print(\"Best feature subset:\", best_result_dt['features'])\n",
    "print(\"Best parameters:\", best_result_dt['best_params'])\n",
    "print(\"Best MAE:\", best_result_dt['best_mae'])\n",
    "print(\"Best MAPE:\", best_result_dt['best_mape'])\n",
    "\n",
    "# Run feature subset evaluation using GPU split CV\n",
    "best_result_dt, res_dt = run_feature_subset_cv(\n",
    "    f_full=f_full,\n",
    "    input_features=input_features,\n",
    "    target_feature=target_feature,\n",
    "    model=model,\n",
    "    model_params=model_params,\n",
    "    output_folder=\"CV_results/input+GPU+prof\",\n",
    "    use_feature_subsets=True,\n",
    "    use_gpu_cv=True  # Set to False to use regular KFold\n",
    ")\n",
    "\n",
    "# Display the best result\n",
    "print(\"Best feature subset:\", best_result_dt['features'])\n",
    "print(\"Best parameters:\", best_result_dt['best_params'])\n",
    "print(\"Best MAE:\", best_result_dt['best_mae'])\n",
    "print(\"Best MAPE:\", best_result_dt['best_mape'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERT tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# Define dataset and parameters\n",
    "input_features = ['N', 'theta', 'dt', 'Clock Rate (MHz)', 'L2 Cache Size (KB)', \n",
    "                  'Multiprocessors (SMs)', 'Half Precision FLOP/s', 'Single Precision FLOP/s', \n",
    "                  'Double Precision FLOP/s'] + selected_features\n",
    "target_feature = 'exec_time_avg'\n",
    "\n",
    "# Define the model pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('normalize', PowerTransformer()),\n",
    "    ('model', ExtraTreesRegressor())\n",
    "])\n",
    "\n",
    "# Wrap with TransformedTargetRegressor\n",
    "model = TransformedTargetRegressor(\n",
    "    regressor=pipeline, \n",
    "    transformer=QuantileTransformer()\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "model_params = {\n",
    "    'regressor__model__n_estimators': [50, 100, 200, 300],        # Number of trees in the forest\n",
    "    'regressor__model__max_depth': [10, 20, 30, None],\n",
    "    'regressor__model__min_samples_split': [2], #, 5, 10\n",
    "    'regressor__model__min_samples_leaf': [1], #, 2, 4\n",
    "    'regressor__model__max_features': [0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Run feature subset evaluation using Random CV\n",
    "best_result_ert, res_ert = run_feature_subset_cv(\n",
    "    f_full=f_full,\n",
    "    input_features=input_features,\n",
    "    target_feature=target_feature,\n",
    "    model=model,\n",
    "    model_params=model_params,\n",
    "    output_folder=\"CV_results/input+GPU+prof\",\n",
    "    use_feature_subsets=True,\n",
    "    use_gpu_cv=False  # Set to False to use regular KFold\n",
    ")\n",
    "\n",
    "# Display the best result\n",
    "print(\"Best feature subset:\", best_result_ert['features'])\n",
    "print(\"Best parameters:\", best_result_ert['best_params'])\n",
    "print(\"Best MAE:\", best_result_ert['best_mae'])\n",
    "print(\"Best MAPE:\", best_result_ert['best_mape'])\n",
    "\n",
    "# Run feature subset evaluation using GPU split CV\n",
    "best_result_ert, res_ert = run_feature_subset_cv(\n",
    "    f_full=f_full,\n",
    "    input_features=input_features,\n",
    "    target_feature=target_feature,\n",
    "    model=model,\n",
    "    model_params=model_params,\n",
    "    output_folder=\"CV_results/input+GPU+prof\",\n",
    "    use_feature_subsets=True,\n",
    "    use_gpu_cv=True  # Set to False to use regular KFold\n",
    ")\n",
    "\n",
    "# Display the best result\n",
    "print(\"Best feature subset:\", best_result_ert['features'])\n",
    "print(\"Best parameters:\", best_result_ert['best_params'])\n",
    "print(\"Best MAE:\", best_result_ert['best_mae'])\n",
    "print(\"Best MAPE:\", best_result_ert['best_mape'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
